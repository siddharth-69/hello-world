{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled40.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharth-69/hello-world/blob/master/Copy_of_Untitled40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MixWN57JMCMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7LUViYQ7ZZmx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load_the_libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import cPickle as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from modelAny import *\n",
        "\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import rnn\n",
        "from tensorflow.python.ops import rnn_cell\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import embedding_ops\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Mp3X82UZrl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env=gym.make('CartPole-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4Ec4W5vZrja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#setting_the_hyperparameters\n",
        "H=8 #no._of_hidden_layer_neurons\n",
        "lr=0.01\n",
        "gamma=0.99\n",
        "decay_rate=0.99 #for_RMS_prop_optimizer\n",
        "resume=False #resume_from_previous_checkpoint\n",
        "\n",
        "model_bs=3\n",
        "real_bs=3\n",
        "\n",
        "#imput_dimensionality\n",
        "D=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDE6C_G3ZrhD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#policy_network\n",
        "tf.reset_default_graph()\n",
        "observations=tf.placeholder(tf.float32,[None,4],name='input_x')\n",
        "W1=tf.get_variable(\"W1\",shape=[4,H],tf.contrib.layers.xavier_initializer())\n",
        "layer1=tf.nn.relu(tf.matmul(observations,W1))\n",
        "W2=tf.get_variable(\"W2\",shape=[H,1],tf.contrib.layers.xavier_initializer())\n",
        "score=tf.nn.relu(tf.matmul(layer1,W2))\n",
        "probability=tf.nn.sigmoid(score)\n",
        "\n",
        "tvar=tf.trainable_variables()\n",
        "input_y=tf.placeholder(tf.float32,[None,1],name=\"input_y\")\n",
        "advantages=tf.placeholder(tf.float32,name=\"reward_signal\")\n",
        "adam=tf.train.AdamOptimizer(learning_rate=lr)\n",
        "W1_grad=tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
        "W2_grad=tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
        "batch_grad=[w1_grad,w2_grad]\n",
        "loglik=tf.log(input_y*(input_y-probability)+(1-input_y)*(input_y+probability))\n",
        "loss=-tf.reduce_mean(loglik*advantages)\n",
        "new_grads=tf.gradients(loss,tvar)\n",
        "update_grads=adam.apply_gradients(zip(batch_grad,tvar))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLcdPDyMZreq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#here_we_implement_the_model_networks\n",
        "mH=256 #model_layer_size\n",
        "input_data=tf.placeholder(tf.float32,[None,5])\n",
        "with tf.variable_scope('rnnlm'):\n",
        "  softmax_w=tf.get_variable(\"softmax_w\",[mH,50])\n",
        "  softmax_b=tf.get_variable(\"softmax_b\",[50])\n",
        "  \n",
        "previous_state=tf.placeholder(tf.float32,[None,5],name=\"prevous_state\")\n",
        "W1M=tf.get_variable(\"W1M\",shape=[5,mH],initializer=tf.contrib.layers.xavier_initializer())\n",
        "B1M=tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
        "layer1M=tf.nn.relu(tf.matmul(previous_state,W1M)+B1M)\n",
        "W2M=tf.get_variable(\"W2M\",shape=[mH,mH],initializer=tf.contrib.layers.xavier_initializer())\n",
        "B2M=tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
        "layer2M=tf.nn.relu(tf.matmul(layer1M,W2M)+B2M)\n",
        "\n",
        "wO=tf.get_variable(\"wO\",shape=[mH,4],initializer=tf.contrib.layers.xavier_initializer())\n",
        "wR=tf.get_variable(\"wR\",shape=[mH,1],initializer=tf.contrib.layers.xavier_initializer())\n",
        "wD=tf.get_variable(\"wD\",shape=[mH,1],initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "bO=tf.Variable(tf.zeros([4]),name=\"bO\")\n",
        "bR=tf.Variable(tf.zeros([1]),name=\"bR\")\n",
        "bD=tf.Variable(tf.zeros([1]),name=\"bD\")\n",
        "\n",
        "predicted_observation=tf.matmul(layer2M,wO,name=\"predicted_observation\")+bO\n",
        "predicted_reward=tf.matmul(layer2M,wR,name=\"predicted_reward\")+bR\n",
        "predicted_done=tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\")+bD)\n",
        "\n",
        "true_observation=tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
        "true_reward=tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
        "true_done=tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
        "\n",
        "predicted_state=tf.concat(1,[predicted_observation,predicted_reward,predicted_done])\n",
        "\n",
        "observation_loss=tf.square(true_observation-predicted_observation)\n",
        "\n",
        "reward_loss=tf.square(true_reward-predicted_reward)\n",
        "\n",
        "done_loss=tf.mul(predicted_done,true_done)+tf.mul(1-predicted_done,1-true_done)\n",
        "done_loss=-tf.log(done_loss)\n",
        "\n",
        "model_loss=tf.reduce_mean(observation_loss+reward_loss+done_loss)\n",
        "\n",
        "adam=tf.train.AdamOptimizer(learning_rate=lr)\n",
        "update_model=adam.minimize(model_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mr1R1scRZrcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#helper_functions\n",
        "\n",
        "def reset_gradbuffer(gradbuffer):\n",
        "  for ix,grad in enumerate(gradbuffer):\n",
        "    gradbuffer[ix]=0\n",
        "  return gradbuffer\n",
        "\n",
        "def discount_rewards(r):\n",
        "  discounted_r=np.zeros_like(r)\n",
        "  running_ad=0\n",
        "  for t in reversed(range(0,r.size)):\n",
        "    running_ad=(running_ad*gamma)+r[t]\n",
        "    discounted_r[t]=running_ad\n",
        "  return discounted_r\n",
        "\n",
        "#this_model_uses_our_model_to_produce_a_new_state_given_an_old_state_and_action.\n",
        "def step_model(sess,xs,action):\n",
        "  to_feed=np.reshape(np.hstack(xs[-1][0],np.array(action)),[1,5])\n",
        "  my_predict=sess.run([predicted_state],feed_dict={previous_state:to_feed})\n",
        "  reward=my_predict[0][:,4]\n",
        "  observation=my_predict[0][:,0:4]\n",
        "  observation[:,0]=np.clip(observation[:,0],-2.4,2.4)\n",
        "  observation[:,2]=np.clip(observation[:,2],-0.4,0.4)\n",
        "  doneP=np.clip(my_predict[0][:,5],0,1)\n",
        "  if doneP>0.1 or len(xs)>=300:\n",
        "    done=True\n",
        "  else:\n",
        "    done=False\n",
        "  return observation,reward,done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2kCUZGqjZrZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training_the_policy_and_the_model\n",
        "\n",
        "xs,drs,ys,ds=[],[],[],[]\n",
        "running_reward=None\n",
        "reward_sum=0\n",
        "episode_number=1\n",
        "real_episodes=1\n",
        "init=tf.initialize_all_variables()\n",
        "batch_size=real_bs\n",
        "\n",
        "draw_from_model=False #when_set_to_true_....will_use_the_model_for_observation\n",
        "\n",
        "train_the_model=True \n",
        "train_the_policy=False\n",
        "\n",
        "switch_point=1\n",
        "\n",
        "#launch_the_graph\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  rendering=False\n",
        "  sess.run(init)\n",
        "  observation=env.reset()\n",
        "  x=observation\n",
        "  grad_buffer=sess.run(tvar)\n",
        "  grad_buffer=reset_gradbuffer(grad_buffer)\n",
        "  while episode_number<=5000:\n",
        "    #start_displaying_environment_once_performance_is_acceptably_high\n",
        "    if (reward_sum/batch_size >150 and draw_from_model==False) or rendering==True:\n",
        "      #HERE_WE_DISPLAY_THE_ENVIRONMENT_WRITE_YOUR_OWN_CODE\n",
        "    \n",
        "    \n",
        "    x=np.reshape(observation,[1,4])\n",
        "    tfprob=sess.run([probability],feed_dict={observations:x})\n",
        "    \n",
        "    action=1 if np.random.uniform()<tfprob else 0\n",
        "    #record_the_intermediates_required_for_back_prop\n",
        "    \n",
        "    y=1 if action==0 else 0\n",
        "    ys.append(y)\n",
        "    \n",
        "    #step_the_model_or_real_environment_and_get_new_measurements\n",
        "    if draw_from_model==False:\n",
        "      observation,reward,done,info=env.step(action)\n",
        "    else:\n",
        "      observation,reward,done=step_model(sess,xs,action)\n",
        "    \n",
        "    reward_sum+=reward\n",
        "    \n",
        "    ds.append(done)\n",
        "    drs.append(reward)  #record_reward...this_is_done_after_we_call_step_to_get_reward_for_previous_action\n",
        "    \n",
        "    if done:\n",
        "      if draw_from_model==False:\n",
        "        real_episode+=1\n",
        "      episode_number+=1\n",
        "      \n",
        "      #stack_together_all_inputs,hidden_states,action_gradients_and_reward_for_this_episode\n",
        "      epx=np.vstack(xs)\n",
        "      epy=np.vstack(ys)\n",
        "      epr=np.vstack(drs)\n",
        "      epd=np.vstack(ds)\n",
        "      \n",
        "      #reset_array_memory\n",
        "      xs,ys,ds,drs=[],[],[],[]\n",
        "      \n",
        "      if train_the_model==True:\n",
        "        actions=np.array([np.abs(y-1) for y in epy][:-1])\n",
        "        state_prevs=epx[:-1,:]\n",
        "        state_prevs=np.hstack([state_prevs,actions])\n",
        "        state_nexts=epx[1:,:]\n",
        "        rewards=np.array(epr[1:,:])\n",
        "        dones=np.array(epd[1:,:])\n",
        "        state_nextsALL=np.hstack([state_nexts,rewards,dones])\n",
        "        \n",
        "        feed_dict={previous_state:state_prevs,true_observation:state_nexts,true_done:dones,true_reward:rewards}\n",
        "        \n",
        "        loss,pState,_=sess.run([model_loss,predicted_state,update_model],feed_dict)\n",
        "        \n",
        "      if train_the_policy==True:\n",
        "        \n",
        "        discounted_epr=discount_rewards(epr).astype('float32')\n",
        "        discounted_epr-=np.mean(discounted_epr)\n",
        "        discounted_epr/=np.std(discounted_epr)\n",
        "        tgrad=sess.run(new_grads,feed_dict={observations:epx,input_y:epy,advantages:discounted_epr})\n",
        "        \n",
        "        #if_gradients_become_too_large_end_training_process\n",
        "        \n",
        "        if np.sum(tgrad[0]==tgrad[0])==0:\n",
        "          break\n",
        "        for ix,grad in enumerate(tgrad):\n",
        "          grad_buffer[ix]+=grad\n",
        "        \n",
        "      if switch_point+batch_size==episode_number:\n",
        "        switch_point=episode_number\n",
        "        if train_the_policy==True:\n",
        "          sess.run(update_grads,feed_dict={W1_grad:grad_buffer[0],W2_grad:grad_buffer[1]})\n",
        "          grad_buffer=reset_gradbuffer(grad_buffer)\n",
        "          running_reward=reward_sum if running_reward is None else ((0.99*running_reward)+(0.01*reward_sum))\n",
        "        if draw_from_model==False:\n",
        "          print('World Perf: Episode %f. Reward %f. action: %f. mean reward %f.'%(real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
        "          \n",
        "          if reward_sum/batch_size>200:\n",
        "            break\n",
        "        \n",
        "        reward_sum=0\n",
        "        \n",
        "        #once_the_model_has_been_trained_on_100_episodes_we_start_alternating_between_training_the_policy_from_the_model_and_training_the_model_from_the_reeal_environment\n",
        "        \n",
        "        if episode_number>100:\n",
        "          draw_from_model=not draw_from_model\n",
        "          train_the_model=not train_the_model\n",
        "          train_the_policy=not train_the_policy\n",
        "          \n",
        "      if draw_from_model==True:\n",
        "        observation=np.random.uniform(-0.1,0.1,[4]) #generate_a_resonable_starting_point\n",
        "        batch_size=model_bs\n",
        "      else:\n",
        "        observation=env.reset()\n",
        "        batch_size=real_bs\n",
        "    \n",
        "print(real_episodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GD1eqOhrZrXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7yT12v-ZrUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BEaME_EQZrSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVUp07TXZrPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}